<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Amber Nguyen" />
    <meta name="description" content="This website includes my projects that I have been working on in SDS348.">
    <link rel="shortcut icon" type="image/x-icon" href="/img/favicon.ico">
    <title>Project 2: Modeling, Testing, and Predicting</title>
    <meta name="generator" content="Hugo 0.61.0" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="/css/main.css" />
    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,400,200bold,400old" />
    
    <!--[if lt IE 9]>
			<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
			<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
		<![endif]-->

    
  </head>

  <body>
    <div id="wrap">

      
      <nav class="navbar navbar-default">
  <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="/"><i class="fa fa-home"></i></a>
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <div class="navbar-collapse collapse" id="navbar">
      <ul class="nav navbar-nav navbar-right">
      
        
        <li><a href="/blog/">BLOG</a></li>
        
        <li><a href="/projects/">PROJECTS</a></li>
        
        <li><a href="/resume/">RESUME</a></li>
        
      
      </ul>
    </div>
  </div>
</nav>

      
      <div class="container">
        <div class="blog-post">
          <h3>
            <strong><a href="/project2/">Project 2: Modeling, Testing, and Predicting</a></strong>
          </h3>
        </div>
        <div class="blog-title">
          <h4>
          January 1, 0001
            &nbsp;&nbsp;
            
          </h4>
        </div>
        <div class="panel panel-default">
          <div class="panel-body">
            <div class="blogpost">
              


<div id="amber-nguyen-aqn246" class="section level1">
<h1>Amber Nguyen aqn246</h1>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>The dataset that I am using for this project is called the Weights of College Students Backpacks. It includes 9 total variables with 100 observations, but the main variables I will be working with are BackpackWeight, BodyWeight, Ratio, BackProblems, Sex, and Year. BackpackWeight is a measure of each student’s backpack weight in pounds. BodyWeight is a measure of each student’s body weight in pounds. Ratio is the ratio of backpack weight to body weight. BackProblems is a binary variable of whether a student has back problems or not, with 0 being no and 1 being yes. Sex is the student’s sex and Year is the student’s year in college. The reason I chose this dataset was because I thought it would be interesting to see if backpack weight has any effect on a college student’s back problems.</p>
</div>
<div id="manova-test" class="section level2">
<h2>1. MANOVA Test</h2>
<pre class="r"><code>mantest &lt;- manova(cbind(BackpackWeight,BodyWeight,Ratio)~BackProblems,data=Backpack)
summary(mantest)</code></pre>
<pre><code>##              Df  Pillai approx F num Df den Df Pr(&gt;F)
## BackProblems  1 0.05201   1.7556      3     96 0.1609
## Residuals    98</code></pre>
<pre class="r"><code>summary.aov(mantest) </code></pre>
<pre><code>## Response BackpackWeight :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## BackProblems 1 46.7 46.707 1.4111 0.2377
## Residuals 98 3243.7 33.099
##
## Response BodyWeight :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## BackProblems 1 1600 1600.2 1.8678 0.1749
## Residuals 98 83957 856.7
##
## Response Ratio :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## BackProblems 1 0.004444 0.0044439 3.3906 0.06859 .
## Residuals 98 0.128443 0.0013106
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>1-.95^4 ## Probability of Type-1 Error</code></pre>
<pre><code>## [1] 0.1854938</code></pre>
<p>The MANOVA test was not significant, thus, for BackpackWeight, BodyWeight, and Ratio, means for students with or without Back Problems did not differ. If the MANOVA test was significant, then I would have performed 4 total tests. 1 MANOVA, 3 univariate ANOVAs, and no t-tests, since my categorical variable only has 2 levels. Lastly, the probability of performing at least one Type-1 Error was 0.1855. There are many assumptions with a MANOVA including: 1. Random samples, independent observations, 2. Multivariate normality, 3. Homogeneity of within-group covariance matrices, 4. Linear relationships among dependent variables, 5. No extreme outliers, and 6. No multicollinearity. I believe that most of the assumptions have been met with the data, as it includes random, independent data. I believe that the variables do not contain any outliers and they are not too correlated, but I am unsure about the covariance matrices, linear relationships and normality. I think it should be formally tested in order to confirm the assumptions as it is usually hard to meet all of these assumptions.</p>
</div>
<div id="randomization-test" class="section level2">
<h2>2. Randomization Test</h2>
<pre class="r"><code>summary(aov(Ratio~BackProblems,data=Backpack))</code></pre>
<pre><code>## Df Sum Sq Mean Sq F value Pr(&gt;F)
## BackProblems 1 0.00444 0.004444 3.391 0.0686 .
## Residuals 98 0.12844 0.001311
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>## Randomization One-Way ANOVA 
obs_F &lt;- 3.391
Fs &lt;- replicate(5000, {
newBP &lt;- Backpack %&gt;% mutate(Ratio = sample(Ratio))
SSW &lt;- newBP %&gt;% group_by(BackProblems) %&gt;% summarize(SSW = sum((Ratio -
mean(Ratio))^2)) %&gt;% summarize(sum(SSW)) %&gt;% pull
SSB &lt;- newBP %&gt;% mutate(mean = mean(Ratio)) %&gt;% group_by(BackProblems) %&gt;%
mutate(groupmean = mean(Ratio)) %&gt;% summarize(SSB = sum((mean -
groupmean)^2)) %&gt;% summarize(sum(SSB)) %&gt;% pull
(SSB/1)/(SSW/98)
})

## Calculated SSB and SSW and F-Value by hand
SSB &lt;- Backpack %&gt;% mutate(mean = mean(Ratio)) %&gt;% group_by(BackProblems) %&gt;%
mutate(groupmean = mean(Ratio)) %&gt;% summarize(SSB = sum((mean -
groupmean)^2)) %&gt;% summarize(sum(SSB)) %&gt;% pull
SSB</code></pre>
<pre><code>## [1] 0.004443907</code></pre>
<pre class="r"><code>SSW &lt;- Backpack %&gt;% group_by(BackProblems) %&gt;% summarize(SSW = sum((Ratio -
mean(Ratio))^2)) %&gt;% summarize(sum(SSW)) %&gt;% pull
SSW</code></pre>
<pre><code>## [1] 0.1284429</code></pre>
<pre class="r"><code>(SSB/1)/(SSW/98) ## F-Value</code></pre>
<pre><code>## [1] 3.390634</code></pre>
<pre class="r"><code>hist(Fs, prob=T); abline(v = obs_F, col=&quot;red&quot;)</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-2-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>mean(Fs&gt;obs_F)</code></pre>
<pre><code>## [1] 0.0686</code></pre>
<p>I performed a randomization one-way ANOVA, to see if there were differences in the mean Ratio (backpackweight/bodyweight) between students with back problems and without back problems. This could help me determine if backpack weight is really the cause of back problems. The null hypothesis is that there is no difference between mean Ratios of students with or without Back Problems. The alternative hypothesis is that there is a difference between mean Ratios of students with or without Back Problems. I ran summary(aov) with the two variables for a comparison to my randomization ANOVA, and it gave a p-value of 0.0686 and an F-value of 3.391. The randomization ANOVA gave very similar results and p-value as my summary(aov) with a p-value of 0.0676. Also, I calculated the SSB and SSW by hand to confirm the F-value and that the randomization code was correct. Overall, the test showed that there are no mean differences in the Backpackweight/Bodyweight Ratio of students with or without back problems as the p-value was not less than 0.05. The histogram of the null distribution shows that the F statistic is around 3.5, and most of the means are around 0. Ths histogram shows that there are not a lot of observations to the right (greater than) the F-statistic, confirming that we cannot reject the null hypothesis.</p>
</div>
<div id="linear-regression" class="section level2">
<h2>3. Linear Regression</h2>
<pre class="r"><code>## Mean-center
Backpack$BodyWeightM &lt;- Backpack$BodyWeight - mean(Backpack$BodyWeight, na.rm=TRUE)

## Linear regression with interaction
fit &lt;- lm(BackpackWeight~BackProblems*BodyWeightM, data=Backpack)
summary(fit)</code></pre>
<pre><code>##
## Call:
## lm(formula = BackpackWeight ~ BackProblems *
BodyWeightM, data = Backpack)
##
## Residuals:
## Min 1Q Median 3Q Max
## -10.1149 -3.7869 -0.8246 3.0606 22.1754
##
## Coefficients:
## Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 11.076846 0.691897 16.009 &lt;2e-16 ***
## BackProblems 1.804529 1.248444 1.445 0.1516
## BodyWeightM 0.041664 0.021750 1.916 0.0584 .
## BackProblems:BodyWeightM -0.003057 0.050368 -0.061
0.9517
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Residual standard error: 5.684 on 96 degrees of freedom
## Multiple R-squared: 0.05732, Adjusted R-squared: 0.02786
## F-statistic: 1.946 on 3 and 96 DF, p-value: 0.1274</code></pre>
<pre class="r"><code>Backpack %&gt;% ggplot(aes(BodyWeightM,BackpackWeight,group=BackProblems, color=BackProblems))+geom_point(aes(color=BackProblems))+geom_smooth(method=&quot;lm&quot;)+ggtitle(&quot;Linear Regression of Bodyweight and Backpack Weight&quot;)+xlab(&quot;Body Weight&quot;)+ylab(&quot;Backpack Weight&quot;)</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-3-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>## Check Assumptions
resids&lt;-fit$residuals
fitted&lt;-fit$fitted.values
ggplot()+geom_point(aes(fitted,resids))+geom_hline(yintercept=0, color=&#39;red&#39;)+ggtitle(&quot;Linearity Test&quot;)+xlab(&quot;Fitted Values&quot;)+ylab(&quot;Residuals&quot;) # Linearity met, Homoskedasticity not met, points fan out</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-3-2.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot()+geom_histogram(aes(resids), bins=20)+ggtitle(&quot;Normality Test&quot;)+xlab(&quot;Residuals&quot;)+ylab(&quot;Count&quot;) # Normality is not good</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-3-3.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>shapiro.test(resids) # Confirms that it is non-normal, reject null hypothesis</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  resids
## W = 0.95376, p-value = 0.00148</code></pre>
<pre class="r"><code>## Robust Standard Errors
fit &lt;- lm(BackpackWeight~BackProblems*BodyWeightM, data=Backpack)
bptest(fit) # this shows that the null hypothesis of homoskedasticity is not rejected. </code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  fit
## BP = 4.3783, df = 3, p-value = 0.2234</code></pre>
<pre class="r"><code>summary(fit) # uncorrect SEs</code></pre>
<pre><code>##
## Call:
## lm(formula = BackpackWeight ~ BackProblems *
BodyWeightM, data = Backpack)
##
## Residuals:
## Min 1Q Median 3Q Max
## -10.1149 -3.7869 -0.8246 3.0606 22.1754
##
## Coefficients:
## Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 11.076846 0.691897 16.009 &lt;2e-16 ***
## BackProblems 1.804529 1.248444 1.445 0.1516
## BodyWeightM 0.041664 0.021750 1.916 0.0584 .
## BackProblems:BodyWeightM -0.003057 0.050368 -0.061
0.9517
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Residual standard error: 5.684 on 96 degrees of freedom
## Multiple R-squared: 0.05732, Adjusted R-squared: 0.02786
## F-statistic: 1.946 on 3 and 96 DF, p-value: 0.1274</code></pre>
<pre class="r"><code>coeftest(fit, vcov = vcovHC(fit)) # correct SEs, did not change much, made the p-value for bodyweight to be not significant. </code></pre>
<pre><code>##
## t test of coefficients:
##
## Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 11.076846 0.693666 15.9686 &lt;2e-16 ***
## BackProblems 1.804529 1.378887 1.3087 0.1938
## BodyWeightM 0.041664 0.027573 1.5111 0.1341
## BackProblems:BodyWeightM -0.003057 0.060583 -0.0505
0.9599
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<p>Controlling for BackProblems, there is no effect of BodyWeight on Backpackweight. For every one unit increase in BodyWeight, Backpack weight increases 0.041664 pounds on average. Controlling for BodyWeight, there is no difference between the Backpackweight of students with or without BackProblems. Additionally, there is no significant interaction between BackProblems and Bodyweight. Regarding assumptions, linearity was met, but the rest of them (Normality, Homoskedasticity) were not as shown with the data above. I ran a bptest to confirm that homoskedasticity was not met. Then, with the robust SEs, the standard errors got larger, which is not good. The p-value of bodyweight also got larger, making it less significant than before. Lastly, the proportion of variation that my model explains is 0.05732 which is very small, if accounting for penalty for each explanatory variable, then the proportion of variation was 0.02786.</p>
</div>
<div id="bootstrapped-ses" class="section level2">
<h2>4. Bootstrapped SEs</h2>
<pre class="r"><code>sampleb &lt;- replicate(5000, {
  bootdata &lt;- bootdata&lt;- Backpack[sample(nrow(Backpack), replace=TRUE),]
  fit &lt;- lm(BackpackWeight~BackProblems*BodyWeightM,data=bootdata)
  coef(fit)
})

sampleb%&gt;%t%&gt;%as.data.frame%&gt;%summarize_all(sd)</code></pre>
<pre><code>## (Intercept) BackProblems BodyWeightM
BackProblems:BodyWeightM
## 1 0.6848684 1.324401 0.02773976 0.05693526</code></pre>
<p>The Bootstrapped SEs got smaller for BackProblems, and the interaction between BackProblems and Bodyweight, but it got larger for Bodyweight in comparison to the Robust SEs. When comparing to the original SEs, none of the Bootstrapped SEs got smaller, but they all got larger. This is not a favorable result, and it shows that the original model is the best in terms of standard errors.</p>
</div>
<div id="logistic-regression" class="section level2">
<h2>5. Logistic Regression</h2>
<pre class="r"><code>fit2 &lt;- glm(BackProblems~BackpackWeight+Ratio, family=&quot;binomial&quot;, data=Backpack)
summary(fit2)</code></pre>
<pre><code>##
## Call:
## glm(formula = BackProblems ~ BackpackWeight + Ratio,
family = &quot;binomial&quot;,
## data = Backpack)
##
## Deviance Residuals:
## Min 1Q Median 3Q Max
## -1.6836 -0.8758 -0.7292 1.2933 1.8234
##
## Coefficients:
## Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -1.5703 0.5415 -2.900 0.00373 **
## BackpackWeight -0.1377 0.1041 -1.323 0.18595
## Ratio 30.9384 16.5032 1.875 0.06084 .
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## (Dispersion parameter for binomial family taken to be 1)
##
## Null deviance: 125.37 on 99 degrees of freedom
## Residual deviance: 120.23 on 97 degrees of freedom
## AIC: 126.23
##
## Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>exp(30.9384)</code></pre>
<pre><code>## [1] 2.731344e+13</code></pre>
<pre class="r"><code>probbp &lt;- predict(fit2, type = &quot;response&quot;) ## predicted probabilities

## Confusion matrix
pred &lt;- ifelse(probbp&gt;0.5,1,0)
table(predict=pred,truth=Backpack$BackProblems)%&gt;%addmargins</code></pre>
<pre><code>##        truth
## predict   0   1 Sum
##     0    66  28  94
##     1     2   4   6
##     Sum  68  32 100</code></pre>
<pre class="r"><code>(66+4)/100 ## Accuracy </code></pre>
<pre><code>## [1] 0.7</code></pre>
<pre class="r"><code>4/32 ## Sensitivity (TPR)</code></pre>
<pre><code>## [1] 0.125</code></pre>
<pre class="r"><code>66/68 ## Specificity (TNR)</code></pre>
<pre><code>## [1] 0.9705882</code></pre>
<pre class="r"><code>4/6 ## Precision (PPV)</code></pre>
<pre><code>## [1] 0.6666667</code></pre>
<pre class="r"><code>## Density plot
Backpack$BackProblems &lt;- as.factor(Backpack$BackProblems)
logitbp &lt;- predict(fit2, type=&quot;link&quot;)
Backpack %&gt;% ggplot()+geom_density(aes(logitbp,fill=BackProblems),alpha=.4)+ggtitle(&quot;Density Plot&quot;)</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-5-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>## ROC Curve
data &lt;- Backpack
data$prob&lt;-predict(fit2,type=&quot;response&quot;) ## predicted probabilities
data$BackProblems&lt;-as.factor(data$BackProblems)
tpr&lt;-function(p)mean(data[data$BackProblems==1,]$prob&gt;p)
fpr&lt;-function(p)mean(data[data$BackProblems==0,]$prob&gt;p)
data&lt;-data[order(data$prob),]
prob&lt;-data$prob
cuts&lt;-unique(c(0,(prob[-1]+prob[-32])/2,1))
TPR&lt;-sapply(cuts,tpr)
FPR&lt;-sapply(cuts,fpr)
ROC1&lt;-
 data.frame(cuts,TPR,FPR,TP=TPR*13,FP=FPR*19)%&gt;%
 arrange(desc(cuts))
ROCplot &lt;- ggplot(ROC1)+geom_path(aes(FPR,TPR),size=1.5)+geom_segment(aes(x=0,y=0,xend=1,yend=1),lty=2)+
 scale_x_continuous(limits = c(0,1))+ggtitle(&quot;ROC Plot&quot;)
ROCplot</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-5-2.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>## Calculate AUC
widths&lt;-diff(ROC1$FPR) 
heights&lt;-(ROC1$TPR[-1]+ROC1$TPR[-length(ROC1$TPR)])/2 
AUC&lt;-sum(heights*widths) 
AUC</code></pre>
<pre><code>## [1] 0.6516544</code></pre>
<pre class="r"><code>## 10-Fold CV
class_diag&lt;-function(probs,truth){
 tab&lt;-table(factor(probs&gt;.5,levels=c(&quot;FALSE&quot;,&quot;TRUE&quot;)),truth)
 acc=sum(diag(tab))/sum(tab)
 sens=tab[2,2]/colSums(tab)[2]
 spec=tab[1,1]/colSums(tab)[1]
 ppv=tab[2,2]/rowSums(tab)[2]
 if(is.numeric(truth)==FALSE &amp; is.logical(truth)==FALSE) truth&lt;-as.numeric(truth)-1
 #CALCULATE EXACT AUC
 ord&lt;-order(probs, decreasing=TRUE)
 probs &lt;- probs[ord]; truth &lt;- truth[ord]
 TPR=cumsum(truth)/max(1,sum(truth))
 FPR=cumsum(!truth)/max(1,sum(!truth))
 dup&lt;-c(probs[-1]&gt;=probs[-length(probs)], FALSE)
 TPR&lt;-c(0,TPR[!dup],1); FPR&lt;-c(0,FPR[!dup],1)
 n &lt;- length(TPR)
 auc&lt;- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
 data.frame(acc,sens,spec,ppv,auc)
} 
set.seed(1234)
k=5 # I used 5 instead of 10 because I only have 100 observations
data1&lt;-data[sample(nrow(data)),] 
folds&lt;-cut(seq(1:nrow(data)),breaks=k,labels=F) 
diags&lt;-NULL
for(i in 1:k){

 train&lt;-data1[folds!=i,]
 test&lt;-data1[folds==i,]
 truth&lt;-test$BackProblems
 
 fit3&lt;-glm(BackProblems~BackpackWeight+Ratio,data=train,family=&quot;binomial&quot;)
 probs&lt;-predict(fit3,newdata = test,type=&quot;response&quot;)

 diags&lt;-rbind(diags,class_diag(probs,truth))
}
apply(diags,2,mean)</code></pre>
<pre><code>##        acc       sens       spec        ppv        auc 
## 0.67000000 0.05357143 0.95618401        NaN 0.61208387</code></pre>
<p>Controlling for Ratio of Backpackweight/Bodyweight, Backpackweight of students with or without Backproblems is not significantly different. Controlling for BackpackWeight, for every 1-unit increase in Ratio, odds of BackProblems increase by a factor of exp(30.9384)=2.731344e+13. The accuracy shows the proportion of correctly classified cases of 0.7, which is not bad. The sensitivity shows the proportion of students with BackProblems correctly classified at 0.125, which is low and not good. The specificity shows the proportion of students without BackProblems correctly classified at 0.971, which is very high. The precision shows the proportion of students classified with BackProblems who actually are at 0.667, which is okay as well. The AUC was calculated to be 0.652, and this is poor, but it is not bad. This shows how well we are predicting a randomly selected person would have BackProblems versus not having BackProblems. I performed a 5-fold CV instead of a 10-Fold, because I only have 100 observations. After performing a 5-Fold CV, The out-of-sample accuracy was 0.690, the sensitivity was 0.115, and the Recall(ppv) was NaN.</p>
</div>
<div id="lasso-regression" class="section level2">
<h2>6. LASSO Regression</h2>
<pre class="r"><code>library(glmnet)
set.seed(1234)
fit4 &lt;- glm(BackProblems~ -1+BackpackWeight+BodyWeight+Ratio+Units+Sex+Major+Year+Status,data=Backpack,family=&quot;binomial&quot;)
y &lt;- as.matrix(Backpack$BackProblems)
x &lt;- model.matrix(fit4)[,-1]
x &lt;- scale(x)
cv&lt;-cv.glmnet(x,y,family=&quot;binomial&quot;)
lasso&lt;-glmnet(x,y,family=&quot;binomial&quot;,lambda=cv$lambda.1se)
coeftest &lt;- coef(lasso)
coeftest</code></pre>
<pre><code>## 48 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                             s0
## (Intercept)      -7.537718e-01
## BodyWeight        .           
## Ratio             .           
## Units             .           
## SexFemale         3.845874e-16
## SexMale          -2.563916e-16
## MajorAero         .           
## MajorAERO         .           
## MajorAero Eng.    .           
## MajorAGB          .           
## MajorAPIO         .           
## MajorARCE         .           
## MajorArch         .           
## MajorBio          .           
## MajorBus          .           
## MajorCD           .           
## MajorCE           .           
## MajorCM           .           
## MajorCPE          .           
## MajorCS           .           
## MajorCSC          .           
## MajorEcon         .           
## MajorEE           .           
## MajorGRC          .           
## MajorHistory      .           
## MajorIE           .           
## MajorIT           .           
## MajorKine         .           
## MajorLS           .           
## MajorMate         .           
## MajorMath         .           
## MajorME           .           
## MajorMFGE         .           
## MajorMLL          .           
## MajorNut.         .           
## MajorNutrition    .           
## MajorPhilosophy   .           
## MajorPhys         .           
## MajorPoli Sci     .           
## MajorPols         .           
## MajorPsy          .           
## MajorPsych        .           
## MajorSoc. Sci.    .           
## MajorSOCS         .           
## MajorSPC          .           
## MajorVocal Music  .           
## Year              .           
## StatusU           .</code></pre>
<pre class="r"><code>## Selecting for Lasso Coefficients
`%ni%`&lt;-Negate(`%in%`)
newlasso &lt;-which(coeftest!=0)
variables&lt;-row.names(coeftest)[newlasso]
variables&lt;-variables[variables %ni% &#39;(Intercept)&#39;] 
select &lt;- dplyr::select
Backpack3 &lt;- x %&gt;% as.data.frame %&gt;% select(variables) %&gt;% mutate(BackProblems = Backpack$BackProblems)

## 10-Fold CV with Lasso
set.seed(1234)
k=5 
data1&lt;-Backpack3[sample(nrow(Backpack)),] 
folds&lt;-cut(seq(1:nrow(Backpack)),breaks=k,labels=F) 
diags&lt;-NULL
for(i in 1:k){
 train&lt;-data1[folds!=i,]
 test&lt;-data1[folds==i,]
 truth&lt;-test$BackProblems
 
 fit&lt;-glm(BackProblems~.,data=train,family=&quot;binomial&quot;)
 probs&lt;-predict(fit,newdata = test,type=&quot;response&quot;)

 diags&lt;-rbind(diags,class_diag(probs,truth))
}
diags%&gt;%summarize_all(mean)</code></pre>
<pre><code>##    acc sens spec ppv       auc
## 1 0.68    0    1 NaN 0.6616758</code></pre>
<p>The variables that were retained from the Lasso regression were SexFemale and SexMale. The 5-fold CV was done and the accuracy was 0.68. This is 0.01 less than the logistic regression without the lasso selected variables, which is not as good. However, the AUC was larger than the original logisitic regression which is good.</p>
</div>
</div>

              <hr>
              <div class="related-posts">
                <h5>Related Posts</h5>
                
              </div>
            </div>
          </div>
          <hr>
        <div class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">

    (function() {
      
      
      if (window.location.hostname == "localhost")
        return;

      var disqus_shortname = '';
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
        </div>
      </div>
      
    </div>

    
    <footer>
  <div id="footer">
    <div class="container">
      <p class="text-muted">&copy; All rights reserved. Powered by <a href="https://gohugo.io/">Hugo</a> and
      <a href="http://www.github.com/nurlansu/hugo-sustain/">sustain</a> with ♥</p>
    </div>
  </div>
</footer>
<div class="footer"></div>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="/js/docs.min.js"></script>
<script src="/js/main.js"></script>

<script src="/js/ie10-viewport-bug-workaround.js"></script>


    
  </body>
</html>
